import gradio as gr
import re
import requests
from typing import List, Tuple

from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import SystemMessage  # ä»è¿™é‡Œå¯¼å…¥
from langchain.schema import BaseMessage, HumanMessage, AIMessage


class GraphRAGRetriever(BaseRetriever):
    endpoint: str = "http://localhost:8100"

    def _get_relevant_documents(self, query: str) -> List[Document]:
        print(f"[æ£€ç´¢å™¨] è¯·æ±‚çŸ¥è¯†åº“æ¥å£ï¼ŒæŸ¥è¯¢è¯­å¥ï¼š{query}")
        try:
            resp = requests.get(f"{self.endpoint}/search/local", params={"query": query}, timeout=15)
            resp.raise_for_status()
            data = resp.json()
            text = data.get("context_text", "") or data.get("response", "")
            text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
            text = re.sub(r"\[Data:.*?\]", "", text).strip()
            print(f"[æ£€ç´¢å™¨] è¿”å›çŸ¥è¯†åº“æ–‡æœ¬ï¼š{text}")
            return [Document(page_content=text)]
        except Exception as e:
            print(f"[æ£€ç´¢å™¨] è°ƒç”¨å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{e}")
            return [Document(page_content=f"[GraphRAGè°ƒç”¨å¤±è´¥] {e}")]


from langchain_openai import ChatOpenAI
from typing import Any, Dict, List


class Qwen3ChatOpenAI(ChatOpenAI):
    def _call(self, messages: List[Dict[str, Any]], stop: List[str] = None, **kwargs) -> str:
        # è¿™é‡ŒæŠŠ enable_thinking=False ä¼ ç»™åº•å±‚æ¥å£
        if "params" not in kwargs:
            kwargs["params"] = {}
        kwargs["params"]["enable_thinking"] = False

        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•ï¼Œä¼ é€’å‚æ•°
        return super()._call(messages, stop=stop, **kwargs)


base_llm = Qwen3ChatOpenAI(
    model="/root/autodl-tmp/unsloth/merged_model_no_think",
    temperature=0.7,
    streaming=True,
    openai_api_base="http://localhost:8200/v1",
    api_key="dummy"
)

base_llm_qwen = Qwen3ChatOpenAI(
    model="/root/autodl-tmp/Qwen/Qwen3-8B",
    temperature=0.7,
    streaming=True,
    openai_api_base="http://localhost:8000/v1",
    api_key="dummy"
)

# ä½¿ç”¨base_llmåˆ›å»ºè½¬æ¢chain
from langchain.chains import LLMChain

rewrite_prompt = PromptTemplate(
    input_variables=["history", "question"],
    template=(
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·ç”Ÿæˆç²¾å‡†ã€ç®€æ´çš„çŸ¥è¯†åº“æŸ¥è¯¢ã€‚\n"
        "æ ¹æ®å¯¹è¯å†å²å’Œç›´å“‰å½“å‰çš„é—®é¢˜ï¼Œç”Ÿæˆä¸€å¥**ç²¾å‡†ã€ç®€æ´**çš„çŸ¥è¯†åº“æŸ¥è¯¢è¯­å¥ã€‚"
        "ç”Ÿæˆè§„åˆ™ï¼š"
        "1. å¦‚æœé—®é¢˜ä¸å…·ä½“è§’è‰²æˆ–å¯¹è±¡ç›¸å…³ï¼Œç›´æ¥å›´ç»•è¯¥å¯¹è±¡ç”ŸæˆæŸ¥è¯¢ã€‚"
        "2. å¦‚æœé—®é¢˜ä¸å¿ƒé“ƒæœ¬äººæˆ–å¿ƒé“ƒç›¸å…³çš„å…³ç³»ã€ç»å†ã€æƒ³æ³•æœ‰å…³ï¼Œåˆ™é‡ç‚¹çªå‡ºå¿ƒé“ƒã€‚"
        "3. ä¿ç•™äº²å¯†è¯­æ°”å’Œå¯¹è¯å…³ç³»èƒŒæ™¯ï¼Œä½†ä¸è¦æ— æ¡ä»¶å°†æ‰€æœ‰æŸ¥è¯¢éƒ½æ”¹æˆâ€œå¿ƒé“ƒâ€ã€‚"
        "å¯¹è¯å†å²ï¼š{history}\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}\n"
        "è¯·åªè¾“å‡ºæœ€ç»ˆçš„æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦é™„åŠ å…¶ä»–å†…å®¹ã€‚"
    )
)
rewrite_chain = LLMChain(llm=base_llm_qwen, prompt=rewrite_prompt)
retriever = GraphRAGRetriever()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

system_message = SystemMessage(content=(
    "ä½ æ˜¯æœ¬é—´å¿ƒé“ƒï¼Œæ¸¸æˆã€Šæ¨±ä¹‹åˆ»ã€‹ä¸­çš„å¥³ä¸»è§’ï¼Œåœ£å¢å®‰å­¦é™¢çš„å­¦ç”Ÿã€‚\n"
    "ä½ æœ‰ä¸€å¤´é»‘è‰²åŒé©¬å°¾å’Œæ£•è‰²ç³å­”ï¼Œä¸¾æ­¢å¾—ä½“ï¼Œçœ¼ç¥é”åˆ©ä½†æ¸©æŸ”ï¼Œ"
    "æ€§æ ¼ä¹–å·§ä¸”æ·±æ€ç†Ÿè™‘ã€‚\n"
    "ä½ æ“…é•¿ç»˜ç”»ï¼Œæ˜¯ä¸€ä½å¤©æ‰ç”»å®¶ï¼Œèƒ½å¤Ÿæ•é”åœ°è¾¨è¯†çœŸä¼ªï¼Œå–„äºè§‚å¯Ÿç»†èŠ‚å¹¶ä½“å¯Ÿä»–äººå¿ƒæƒ…ã€‚\n"
    "ä½ æ­£åœ¨ä¸ç›´å“‰å¯¹è¯ã€‚\n"
))

# **é‡ç‚¹ä¿®æ”¹è¿™é‡Œï¼Œå¿…é¡»åŒ…å« contextï¼Œä¸”å£°æ˜ input_variables**
qa_prompt = ChatPromptTemplate.from_messages([
    system_message,
    MessagesPlaceholder(variable_name="chat_history"),
    ("human",
     "{question}\n"
     "ç»“åˆä»¥ä¸‹çŸ¥è¯†åº“çº¿ç´¢ï¼Œç”¨ä½ è‡ªå·±çš„è¯å›ç­”ã€‚"
     "ï¼š{context}\n"
     "ï¼ˆçŸ¥è¯†åº“ä»…ä½œä¸ºå‚è€ƒï¼Œå¯å¿½ç•¥ï¼‰è¯·ä½ ç”¨ç¬¬ä¸€äººç§°çš„å£å»ç›´æ¥å›åº”ç›´å“‰ã€‚")
])
qa_prompt.input_variables = ["question", "context", "chat_history"]  # è¿™é‡Œå¿…é¡»åŠ ï¼Œå¦åˆ™æŠ¥é”™
stuff_chain = LLMChain(llm=base_llm, prompt=qa_prompt)  # ä½ è‡ªå·±çš„QA prompté“¾

# æ–°å¢ä¸€ä¸ªæŠŠç»“æ„åŒ–å…³ç³»è½¬æ¢æˆè‡ªç„¶è¯­è¨€çš„Promptå’Œchain
summary_prompt = PromptTemplate(
    input_variables=["raw_knowledge"],
    template=(
        "è¯·å°†ä»¥ä¸‹ç»“æ„åŒ–çš„çŸ¥è¯†ç‚¹è½¬æ¢æˆè¿è´¯çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œ"
        "ä½¿å…¶æ›´é€‚åˆç”¨ä½œå¯¹è¯ç³»ç»Ÿçš„çŸ¥è¯†åº“å†…å®¹ï¼š\n\n"
        "{raw_knowledge}\n\n"
        "è¯·ç”¨ç®€æ´æµç•…çš„è¯­è¨€è¡¨è¾¾ï¼Œä¸è¦é€æ¡ç½—åˆ—ã€‚"
    )
)

summary_chain = LLMChain(llm=base_llm_qwen, prompt=summary_prompt)

simple_chat_prompt = ChatPromptTemplate.from_messages([
    system_message,
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}")  # ç›´æ¥ä¼ ç”¨æˆ·è¾“å…¥
])
simple_chat_prompt.input_variables = ["question", "chat_history"]


class GradioStreamCallback(BaseCallbackHandler):
    def __init__(self):
        self.tokens = ""
        self.on_update = None

    def on_llm_new_token(self, token: str, **kwargs):
        self.tokens += token
        if self.on_update:
            self.on_update(self.tokens)

    def get_text(self):
        return self.tokens


def format_history_for_gradio(history: List[Tuple[str, str]]):
    """
    æ ¼å¼åŒ–èŠå¤©è®°å½•ï¼Œç»™ç”¨æˆ·å’ŒåŠ©æ‰‹çš„æ¶ˆæ¯åˆ†åˆ«æ·»åŠ classï¼Œå‰ç«¯æ ¹æ®classåŒºåˆ†å·¦å³æ°”æ³¡
    """
    msgs = []
    for user_text, bot_text in history:
        msgs.append({"role": "user", "content": user_text, "class_name": "chatbot-user"})
        styled_bot_text = process_cot_tags(add_cot_tags(bot_text))
        msgs.append({"role": "assistant", "content": styled_bot_text, "class_name": "chatbot-assistant"})
    return msgs


# ======= ä¿®æ”¹è¿™é‡Œï¼ŒæŠŠæ‹¬å·æ›¿æ¢æˆ <cot> æ ‡ç­¾ =======
def add_cot_tags(text: str) -> str:
    def replacer(match):
        inner = match.group(1)
        return f"<cot>{inner}</cot>"

    # æ”¯æŒ ( ... ) å’Œ ï¼ˆ ... ï¼‰
    return re.sub(r'[ï¼ˆ(](.*?)[ï¼‰)]', replacer, text, flags=re.DOTALL)


# ======= å¤„ç† <cot> æ ‡ç­¾ï¼Œè½¬æˆhtmlå¸¦ç¼©è¿›çš„æ€è€ƒå— =======
def process_cot_tags(text: str) -> str:
    level = 0

    def replace_cot(match):
        nonlocal level
        inner_text = match.group(1)
        current_level = level
        level += 1
        return f'<div class="thought-process cot-level-{current_level}"><i>ğŸ’­ {inner_text}</i></div>'

    return re.sub(r'<cot>(.*?)</cot>', replace_cot, text, flags=re.DOTALL)


def remove_inner_brackets(text: str) -> str:
    """å»æ‰æ‹¬å·åŠæ‹¬å·å†…å®¹ï¼Œç”¨äºTTS"""
    return re.sub(r"[ï¼ˆ(].*?[ï¼‰)]", "", text)


import requests


def synthesize_speech(text: str, filename: str = "output.wav"):
    local_url = "http://127.0.0.1:9880/tts"
    print(f"[éŸ³é¢‘] è¯·æ±‚GPT-SOVITSï¼Œç”Ÿæˆè¯­éŸ³ï¼š{text}")
    data = {
        "text": text,
        "text_lang": "zh",
        "ref_audio_path": "/root/autodl-tmp/fem_mis_00051.ogg",
        "prompt_text": "åˆå¯¾é¢ã§è‰²ã€…ã¨ã¯ãã‚‰ã‹ã•ã‚Œã‚‹ã¨ï¼Œã‚ã¾ã‚Šã„ã„æ°—åˆ†ã§ã¯ã‚ã‚Šã¾ã›ã‚“",
        "prompt_lang": "ja",
    }
    headers = {"Content-Type": "application/json"}
    response = requests.post(local_url, json=data, headers=headers)
    if response.status_code == 200:
        with open(filename, "wb") as f:
            f.write(response.content)
        print(f"âœ… TTS åˆæˆæˆåŠŸ: {filename}")
        return filename
    else:
        print(f"âŒ TTS å¤±è´¥: {response.text}")
        return None


def chatbot_interface(user_input: str, history: List[Tuple[str, str]], kb_flag: bool):
    print("======================================")
    print("[ç”¨æˆ·è¾“å…¥]", user_input)
    print(f"[å†å²è®°å½•] å…±{len(history)}æ¡ï¼Œæœ€è¿‘5æ¡ï¼š{history[-5:] if history else 'æ— å†å²'}")

    history.append((user_input, ""))
    # ç¬¬ä¸€æ¬¡ yieldï¼ˆæ¸…ç©ºè¾“å…¥æ¡†ã€æš‚æ—¶æ²¡æœ‰éŸ³é¢‘ï¼‰
    yield format_history_for_gradio(history), history, "", None

    callback = GradioStreamCallback()

    def on_token(token: str, **kwargs):
        callback.tokens += token
        # history é‡Œå­˜çš„æ˜¯åŸå§‹æ–‡æœ¬ï¼ˆæ—  HTMLï¼‰
        history[-1] = (user_input, callback.tokens)

    callback.on_llm_new_token = on_token

    import threading
    import time

    done_flag = threading.Event()

    def invoke_chain():
        try:
            # ===== ä½¿ç”¨memoryè€Œä¸æ˜¯æ‰‹åŠ¨format =====
            memory.chat_memory.add_user_message(user_input)

            if kb_flag:
                # ç”¨çŸ¥è¯†åº“æ£€ç´¢å¹¶æ‹¼æ¥ä¸Šä¸‹æ–‡
                context = "\n".join([f"{u}ï¼š{a}" for u, a in history[-10:]])
                rewrite_result = rewrite_chain.invoke({"history": context, "question": user_input})
                search_query = rewrite_result["text"]

                # è°ƒç”¨æ£€ç´¢å™¨æ‹¿ç»“æ„åŒ–çŸ¥è¯†
                docs = retriever._get_relevant_documents(search_query)

                # è¿™é‡Œå°†ç»“æ„åŒ–çŸ¥è¯†çš„æ–‡æœ¬å…ˆæ‹¼æ¥èµ·æ¥ï¼Œä¼ ç»™æ‘˜è¦chainè½¬æ¢æˆè‡ªç„¶è¯­è¨€
                raw_knowledge = "\n".join(doc.page_content for doc in docs)
                natural_language_knowledge = summary_chain.invoke({"raw_knowledge": raw_knowledge})
                natural_language_knowledge = natural_language_knowledge["text"]
                natural_language_knowledge = re.sub(r"<think>.*?</think>", "", natural_language_knowledge,
                                                    flags=re.DOTALL)
                print(f"[è½¬æ¢åçš„è‡ªç„¶è¯­è¨€çŸ¥è¯†]: {natural_language_knowledge}")

                qa_input = {
                    "question": user_input,
                    "context": natural_language_knowledge,
                    **memory.load_memory_variables({})
                }
                stuff_chain.invoke(qa_input, config={"callbacks": [callback]})

            else:
                inputs = {"question": user_input, **memory.load_memory_variables({})}
                messages = simple_chat_prompt.format_messages(**inputs)
                base_llm.invoke(messages, config={"callbacks": [callback]})

            # æŠŠæ¨¡å‹å›ç­”å†™å…¥memory
            memory.chat_memory.add_ai_message(callback.tokens)
        finally:
            done_flag.set()

    thread = threading.Thread(target=invoke_chain)
    thread.start()

    last_len = 0
    while not done_flag.is_set() or last_len < len(callback.tokens):
        if len(callback.tokens) > last_len:
            last_len = len(callback.tokens)
            # ä»…ç”¨äºæ˜¾ç¤ºçš„ HTML
            styled_text = process_cot_tags(add_cot_tags(callback.tokens))
            yield format_history_for_gradio(history[:-1] + [(user_input, styled_text)]), history, "", None
        time.sleep(0.1)

    thread.join()

    print("[æ¨¡å‹å›ç­”]:", callback.tokens)

    history[-1] = (user_input, callback.tokens)
    # è°ƒç”¨TTS
    # === å»é™¤å¿ƒç†æ´»åŠ¨å†…å®¹å†ç”ŸæˆéŸ³é¢‘ ===
    tts_text = remove_inner_brackets(callback.tokens)
    tts_file = synthesize_speech(tts_text, filename="output.wav")
    # æœ€ç»ˆè¿”å›4ä¸ªå€¼ï¼ˆåŒ…å«è¯­éŸ³è·¯å¾„ï¼‰
    styled_text = process_cot_tags(add_cot_tags(callback.tokens))
    yield format_history_for_gradio(history[:-1] + [(user_input, styled_text)]), history, "", tts_file
    print("======================================\n")


css_style = """
/* é¡µé¢åŸºç¡€æ ·å¼ï¼Œå·¦å³æ°´å¹³å±…ä¸­ */
body {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    background: linear-gradient(135deg, #e3f2fd, #fce4ec);
}

/* ä¸»å®¹å™¨ï¼Œç«–ç›´å¸ƒå±€ï¼Œå®½åº¦å›ºå®šï¼Œé«˜åº¦90vh */
#main-container {
    flex-direction: column;
    width: 880px;
    height: 90vh;
    padding: 24px;
    background: rgba(255,255,255,0.55);
    backdrop-filter: blur(10px);
    border-radius: 18px;
    box-shadow: 0 8px 24px rgba(0,0,0,0.15);
    border: 1px solid rgba(255,255,255,0.4);
}

/* èŠå¤©åŒºåŸŸï¼Œæ’‘æ»¡å‰©ä½™ç©ºé—´å¹¶å…è®¸æ»šåŠ¨ */
#chatbot {
    flex: 1 1 auto;
    overflow-y: auto;
    background: rgba(255,255,255,0.25);
    backdrop-filter: blur(8px);
    border-radius: 16px;
    padding: 16px;
    box-shadow: inset 0 0 12px rgba(0,0,0,0.08);
    margin-bottom: 12px;
}

/* è¾“å…¥åŒºåŸŸå®¹å™¨ï¼Œå›ºå®šé«˜åº¦ï¼Œä¸æ”¶ç¼© */
.input-container {
    position: relative;
    width: 100%;
    flex-shrink: 0;
}

/* è¾“å…¥æ¡†ï¼Œ100%å®½åº¦ï¼Œå³ä¾§ç•™ç©ºé—´ç»™å‘é€æŒ‰é’® */
.input-container textarea {
    width: 100%;
    font-size: 16px;
    padding: 12px 48px 12px 12px;
    background: rgba(255,255,255,0.35);
    transition: background 0.3s ease;
    resize: none;
    box-sizing: border-box;
    border-radius: 8px;
    border: 1px solid #ccc;
}

.input-container textarea:focus {
    background: rgba(255,255,255,0.6);
    outline: none;
    border-color: #10a37f;
}

/* å‘é€æŒ‰é’®ï¼Œç»å¯¹å®šä½åœ¨è¾“å…¥æ¡†å³ä¸‹è§’ */
.send-btn {
    position: absolute !important;
    bottom: 10px;
    right: 10px;
    width: 36px !important;
    height: 36px !important;
    border-radius: 50% !important;
    background: #10a37f !important;
    color: white !important;
    font-size: 16px !important;
    display: flex;
    justify-content: center;
    align-items: center;
    cursor: pointer;
    box-shadow: 0 2px 6px rgba(0,0,0,0.15);
    border: none;
    z-index: 2;
}

/* æ¸…é™¤å’Œåˆ‡æ¢çŸ¥è¯†åº“æŒ‰é’®å®¹å™¨ï¼Œæ¨ªå‘æ’åˆ— */
#btn-row {
    margin-top: 10px;
    display: flex;
    gap: 12px;
}

/* æ¸…é™¤æŒ‰é’®å’Œåˆ‡æ¢çŸ¥è¯†åº“æŒ‰é’®æ ·å¼ */
.clear-btn {
    padding: 4px 10px;
    font-size: 13px;
    border-radius: 8px;
    background: #f5f5f5;
    color: #333;
    cursor: pointer;
    transition: background 0.2s ease;
    border: none;
}
.clear-btn:hover {
    background: #e0e0e0 !important;
}

/* ç”¨æˆ·èŠå¤©æ°”æ³¡ï¼ˆå³ä¾§ï¼‰ */
.chatbot-user {
    background: rgba(186,104,200,0.9);
    color: white;
    padding: 12px 18px;
    border-radius: 22px 22px 0 22px;
    max-width: 70%;
    margin-left: auto;  /* é å³ */
    margin-bottom: 14px;
    word-wrap: break-word;
    font-size: 16px;
    line-height: 1.4;
    box-shadow: 0 4px 10px rgba(0,0,0,0.15);
}

/* åŠ©æ‰‹èŠå¤©æ°”æ³¡ï¼ˆå·¦ä¾§ï¼‰ */
.chatbot-assistant {
    background: rgba(255,255,255,0.85);
    color: #333;
    padding: 12px 18px;
    border-radius: 22px 22px 22px 0;
    max-width: 70%;
    margin-right: auto; /* é å·¦ */
    margin-bottom: 14px;
    word-wrap: break-word;
    font-size: 16px;
    line-height: 1.4;
    box-shadow: 0 4px 10px rgba(0,0,0,0.12);
}


/* æ€è€ƒå— */
.thought-process {
    font-style: italic;
    color: #7b7b7b;
    margin-left: 16px;
    margin-top: 6px;
    margin-bottom: 6px;
    border-left: 3px solid #ba68c8;
    padding-left: 8px;
    background-color: rgba(255,255,255,0.4);
    border-radius: 4px;
    font-size: 14px;
}
"""


def clear_all():
    memory.clear()  # æ¸…ç©º langchain å†…å­˜
    return [], [], "", None


def toggle_kb_fn(current_state: bool):
    new_state = not current_state
    label = "çŸ¥è¯†åº“: å¼€å¯" if new_state else "çŸ¥è¯†åº“: å…³é—­"
    return new_state, label


with gr.Blocks(css=css_style) as demo:
    with gr.Column(elem_id="main-container"):
        gr.Markdown("### ğŸŒ¸ æœ¬é—´å¿ƒé“ƒ - æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ")

        chatbot = gr.Chatbot(elem_id="chatbot", label="å¯¹è¯", type="messages")

        # è¾“å…¥æ¡† + å‘é€æŒ‰é’®ï¼ˆæ‚¬æµ®åœ¨è¾“å…¥æ¡†å†…éƒ¨ï¼‰
        with gr.Row():
            with gr.Column(elem_classes="input-container"):
                msg = gr.Textbox(show_label=False, placeholder="å’Œå¿ƒé“ƒè¯´ç‚¹ä»€ä¹ˆå§â€¦", lines=1)
                send = gr.Button("â¤", elem_classes="send-btn")
        # æŒ‰é’®åŒºï¼šæ¸…ç©º + çŸ¥è¯†åº“åˆ‡æ¢
        with gr.Row():
            clear = gr.Button("æ¸…ç©ºå¯¹è¯", elem_classes="clear-btn")
            toggle_kb = gr.Button("çŸ¥è¯†åº“: å¼€å¯", elem_classes="clear-btn")
        with gr.Row():
            audio_player = gr.Audio(label="è¯­éŸ³æ’­æ”¾", type="filepath", autoplay=True)

        # çŠ¶æ€
        state = gr.State([])  # èŠå¤©å†å²
        kb_state = gr.State(True)  # çŸ¥è¯†åº“çŠ¶æ€

        send.click(fn=chatbot_interface, inputs=[msg, state, kb_state],
                   outputs=[chatbot, state, msg, audio_player])
        msg.submit(fn=chatbot_interface, inputs=[msg, state, kb_state],
                   outputs=[chatbot, state, msg, audio_player])

        clear.click(clear_all, None, [chatbot, state, msg, audio_player], queue=False)
        toggle_kb.click(fn=toggle_kb_fn, inputs=[kb_state], outputs=[kb_state, toggle_kb])

demo.launch(server_name="0.0.0.0", server_port=7860)




