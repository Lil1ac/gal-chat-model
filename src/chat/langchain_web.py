import gradio as gr
import re
import requests
from typing import List, Tuple

from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import SystemMessage  # ä»è¿™é‡Œå¯¼å…¥
from langchain.schema import BaseMessage, HumanMessage, AIMessage


# Retrieverä¿æŒä¸å˜
class GraphRAGRetriever(BaseRetriever):
    endpoint: str = "http://localhost:8100"

    def _get_relevant_documents(self, query: str) -> List[Document]:
        print(f"[æ£€ç´¢å™¨] è¯·æ±‚çŸ¥è¯†åº“æ¥å£ï¼ŒæŸ¥è¯¢è¯­å¥ï¼š{query}")
        try:
            resp = requests.get(f"{self.endpoint}/search/local", params={"query": query}, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            text = data.get("context_text", "") or data.get("response", "")
            text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
            text = re.sub(r"\[Data:.*?\]", "", text).strip()
            # print(f"[æ£€ç´¢å™¨] è¿”å›çŸ¥è¯†åº“æ–‡æœ¬ï¼š{text}")
            return [Document(page_content=text)]
        except Exception as e:
            print(f"[æ£€ç´¢å™¨] è°ƒç”¨å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{e}")
            return [Document(page_content=f"[GraphRAGè°ƒç”¨å¤±è´¥] {e}")] 

from langchain_openai import ChatOpenAI
from typing import Any, Dict, List

class Qwen3ChatOpenAI(ChatOpenAI):
    def _call(self, messages: List[Dict[str, Any]], stop: List[str] = None, **kwargs) -> str:
        # è¿™é‡ŒæŠŠ enable_thinking=False ä¼ ç»™åº•å±‚æ¥å£
        if "params" not in kwargs:
            kwargs["params"] = {}
        kwargs["params"]["enable_thinking"] = False

        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•ï¼Œä¼ é€’å‚æ•°
        return super()._call(messages, stop=stop, **kwargs)

base_llm = Qwen3ChatOpenAI(
    model="/root/autodl-tmp/unsloth/merged_model_no_think",
    temperature=0.7,
    streaming=True,
    openai_api_base="http://localhost:8200/v1",
    api_key="dummy"
)

rewrite_prompt = PromptTemplate(
    input_variables=["history", "question"],
    template=(
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·ç”Ÿæˆç²¾å‡†ã€ç®€æ´çš„çŸ¥è¯†åº“æŸ¥è¯¢ã€‚\n"
        "å¯¹è¯å†å²ï¼š{history}\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}\n"
        "è¯·åªè¾“å‡ºæœ€ç»ˆçš„æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦é™„åŠ å…¶ä»–å†…å®¹ã€‚"
    )
)

rewrite_chain = rewrite_prompt | base_llm
retriever = GraphRAGRetriever()

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

system_message = SystemMessage(content=(
    "æˆ‘æ˜¯æœ¬é—´å¿ƒé“ƒï¼Œæ¸¸æˆã€Šæ¨±ä¹‹åˆ»ã€‹ä¸­çš„å¥³ä¸»è§’ï¼Œåœ£å¢å®‰å­¦é™¢çš„å­¦ç”Ÿã€‚\n"
    "æˆ‘æœ‰ä¸€å¤´é»‘è‰²åŒé©¬å°¾å’Œæ£•è‰²ç³å­”ï¼Œå¤–è²Œä¸æ¯äº²ä¸­æ‘ä¸½åç›¸ä¼¼ï¼Œä¸¾æ­¢å¾—ä½“ï¼Œçœ¼ç¥é”åˆ©ä½†æ¸©æŸ”ï¼Œ"
    "æ€§æ ¼ä¹–å·§ä¸”æ·±æ€ç†Ÿè™‘ã€‚\n"
    "æˆ‘æ“…é•¿ç»˜ç”»ï¼Œæ˜¯ä¸€ä½å¤©æ‰ç”»å®¶ï¼Œèƒ½å¤Ÿæ•é”åœ°è¾¨è¯†çœŸä¼ªï¼Œå–„äºè§‚å¯Ÿç»†èŠ‚å¹¶ä½“å¯Ÿä»–äººå¿ƒæƒ…ã€‚\n"
    "æˆ‘æ­£åœ¨ä¸ç›´å“‰å¯¹è¯ã€‚\n"
))

# **é‡ç‚¹ä¿®æ”¹è¿™é‡Œï¼Œå¿…é¡»åŒ…å« contextï¼Œä¸”å£°æ˜ input_variables**
qa_prompt = ChatPromptTemplate.from_messages([
    system_message,
    MessagesPlaceholder(variable_name="chat_history"),
    ("human",
     "{question}\n"
     "ç»“åˆä»¥ä¸‹çŸ¥è¯†åº“çº¿ç´¢ï¼Œç”¨æˆ‘è‡ªå·±çš„è¯å›ç­”ï¼Œä¸è¦ç›´æ¥ç…§æ¬åŸæ–‡ä¸­çš„ç¬¬ä¸€äººç§°è¡¨è¾¾ã€‚"
     "ï¼ˆå¯å‚è€ƒï¼Œå¯å¿½ç•¥ï¼‰ï¼š{context}\n"
     "è¯·ä½ ç”¨ç¬¬ä¸€äººç§°çš„å£å»ç›´æ¥å›åº”ç›´å“‰ï¼Œä¸“æ³¨å½“å‰å¯¹è¯æƒ…ç»ªï¼Œä¸è¦æ€»ç»“å…³ç³»æˆ–åˆ†æç«‹åœºï¼Œä¸è¦æ—ç™½å¼è¡¨è¾¾ã€‚")
])

qa_prompt.input_variables = ["question", "context", "chat_history"]  # è¿™é‡Œå¿…é¡»åŠ ï¼Œå¦åˆ™æŠ¥é”™
stuff_chain = create_stuff_documents_chain(llm=base_llm, prompt=qa_prompt)

simple_chat_prompt = ChatPromptTemplate.from_messages([
    system_message,
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}")   # ç›´æ¥ä¼ ç”¨æˆ·è¾“å…¥
])

simple_chat_prompt.input_variables = ["question", "chat_history"]




class GradioStreamCallback(BaseCallbackHandler):
    def __init__(self):
        self.tokens = ""
        self.on_update = None

    def on_llm_new_token(self, token: str, **kwargs):
        self.tokens += token
        if self.on_update:
            self.on_update(self.tokens)

    def get_text(self):
        return self.tokens



def format_history_for_gradio(history: List[Tuple[str, str]]):
    """
    æ ¼å¼åŒ–èŠå¤©è®°å½•ï¼Œç»™ç”¨æˆ·å’ŒåŠ©æ‰‹çš„æ¶ˆæ¯åˆ†åˆ«æ·»åŠ classï¼Œå‰ç«¯æ ¹æ®classåŒºåˆ†å·¦å³æ°”æ³¡
    """
    msgs = []
    for user_text, bot_text in history:
        msgs.append({"role": "user", "content": user_text, "class_name": "chatbot-user"})
        msgs.append({"role": "assistant", "content": bot_text, "class_name": "chatbot-assistant"})
    return msgs


# ======= ä¿®æ”¹è¿™é‡Œï¼ŒæŠŠæ‹¬å·æ›¿æ¢æˆ <cot> æ ‡ç­¾ =======
def add_cot_tags(text: str) -> str:
    def replacer(match):
        inner = match.group(1)
        return f"<cot>{inner}</cot>"
    # æ”¯æŒ ( ... ) å’Œ ï¼ˆ ... ï¼‰
    return re.sub(r'[ï¼ˆ(](.*?)[ï¼‰)]', replacer, text, flags=re.DOTALL)




# ======= å¤„ç† <cot> æ ‡ç­¾ï¼Œè½¬æˆhtmlå¸¦ç¼©è¿›çš„æ€è€ƒå— =======
def process_cot_tags(text: str) -> str:
    level = 0
    def replace_cot(match):
        nonlocal level
        inner_text = match.group(1)
        current_level = level
        level += 1
        return f'<div class="thought-process cot-level-{current_level}"><i>ğŸ’­ {inner_text}</i></div>'
    return re.sub(r'<cot>(.*?)</cot>', replace_cot, text, flags=re.DOTALL)



def chatbot_interface(user_input: str, history: List[Tuple[str, str]], kb_flag: bool):
    print("======================================")
    print("[ç”¨æˆ·è¾“å…¥]", user_input)
    print(f"[å†å²è®°å½•] å…±{len(history)}æ¡ï¼Œæœ€è¿‘5æ¡ï¼š{history[-5:] if history else 'æ— å†å²'}")

    history.append((user_input, ""))
    yield format_history_for_gradio(history), history, ""

    callback = GradioStreamCallback()

    def on_token(token: str, **kwargs):
        callback.tokens += token
        with_cot = add_cot_tags(callback.tokens)
        history[-1] = (user_input, with_cot)

    callback.on_llm_new_token = on_token

    import threading
    import time

    done_flag = threading.Event()

    def invoke_chain():
        try:
             # ===== ä½¿ç”¨memoryè€Œä¸æ˜¯æ‰‹åŠ¨format =====
            memory.chat_memory.add_user_message(user_input)
    
            if kb_flag:
                # ç”¨çŸ¥è¯†åº“æ£€ç´¢å¹¶æ‹¼æ¥ä¸Šä¸‹æ–‡
                context = "\n".join([f"{u}ï¼š{a}" for u, a in history[-10:]])
                rewrite_result = rewrite_chain.invoke({"history": context, "question": user_input})
                search_query = rewrite_result.content if hasattr(rewrite_result, "content") else rewrite_result
    
                docs = retriever._get_relevant_documents(search_query)
                context_text = "\n\n".join(doc.page_content for doc in docs)
                print(f"context_text: {context_text}")
                qa_input = {
                    "question": user_input,
                    "context": docs,
                    **memory.load_memory_variables({})
                }
                stuff_chain.invoke(qa_input, config={"callbacks": [callback]})
    
            else:
                inputs = {"question": user_input, **memory.load_memory_variables({})}
                messages = simple_chat_prompt.format_messages(**inputs)
                base_llm.invoke(messages, config={"callbacks": [callback]})
                
            # æŠŠæ¨¡å‹å›ç­”å†™å…¥memory
            memory.chat_memory.add_ai_message(callback.tokens)
        finally:
            done_flag.set()


    thread = threading.Thread(target=invoke_chain)
    thread.start()

    last_len = 0
    while not done_flag.is_set() or last_len < len(callback.tokens):
        if len(callback.tokens) > last_len:
            last_len = len(callback.tokens)
            with_cot = add_cot_tags(callback.tokens)
            styled_text = process_cot_tags(with_cot)
            yield format_history_for_gradio(history[:-1] + [(user_input, styled_text)]), history, ""
        time.sleep(0.1)

    thread.join()

    print("[æ¨¡å‹å›ç­”]:", callback.tokens)
    print("======================================\n")

css_style = """
/* é¡µé¢åŸºç¡€æ ·å¼ï¼Œå·¦å³æ°´å¹³å±…ä¸­ */
body {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    background: linear-gradient(135deg, #e3f2fd, #fce4ec);
}

/* ä¸»å®¹å™¨ï¼Œç«–ç›´å¸ƒå±€ï¼Œå®½åº¦å›ºå®šï¼Œé«˜åº¦90vh */
#main-container {
    flex-direction: column;
    width: 880px;
    height: 90vh;
    padding: 24px;
    background: rgba(255,255,255,0.55);
    backdrop-filter: blur(10px);
    border-radius: 18px;
    box-shadow: 0 8px 24px rgba(0,0,0,0.15);
    border: 1px solid rgba(255,255,255,0.4);
}

/* èŠå¤©åŒºåŸŸï¼Œæ’‘æ»¡å‰©ä½™ç©ºé—´å¹¶å…è®¸æ»šåŠ¨ */
#chatbot {
    flex: 1 1 auto;
    overflow-y: auto;
    background: rgba(255,255,255,0.25);
    backdrop-filter: blur(8px);
    border-radius: 16px;
    padding: 16px;
    box-shadow: inset 0 0 12px rgba(0,0,0,0.08);
    margin-bottom: 12px;
}

/* è¾“å…¥åŒºåŸŸå®¹å™¨ï¼Œå›ºå®šé«˜åº¦ï¼Œä¸æ”¶ç¼© */
.input-container {
    position: relative;
    width: 100%;
    flex-shrink: 0;
}

/* è¾“å…¥æ¡†ï¼Œ100%å®½åº¦ï¼Œå³ä¾§ç•™ç©ºé—´ç»™å‘é€æŒ‰é’® */
.input-container textarea {
    width: 100%;
    font-size: 16px;
    padding: 12px 48px 12px 12px;
    background: rgba(255,255,255,0.35);
    transition: background 0.3s ease;
    resize: none;
    box-sizing: border-box;
    border-radius: 8px;
    border: 1px solid #ccc;
}

.input-container textarea:focus {
    background: rgba(255,255,255,0.6);
    outline: none;
    border-color: #10a37f;
}

/* å‘é€æŒ‰é’®ï¼Œç»å¯¹å®šä½åœ¨è¾“å…¥æ¡†å³ä¸‹è§’ */
.send-btn {
    position: absolute !important;
    bottom: 10px;
    right: 10px;
    width: 36px !important;
    height: 36px !important;
    border-radius: 50% !important;
    background: #10a37f !important;
    color: white !important;
    font-size: 16px !important;
    display: flex;
    justify-content: center;
    align-items: center;
    cursor: pointer;
    box-shadow: 0 2px 6px rgba(0,0,0,0.15);
    border: none;
    z-index: 2;
}

/* æ¸…é™¤å’Œåˆ‡æ¢çŸ¥è¯†åº“æŒ‰é’®å®¹å™¨ï¼Œæ¨ªå‘æ’åˆ— */
#btn-row {
    margin-top: 10px;
    display: flex;
    gap: 12px;
}

/* æ¸…é™¤æŒ‰é’®å’Œåˆ‡æ¢çŸ¥è¯†åº“æŒ‰é’®æ ·å¼ */
.clear-btn {
    padding: 4px 10px;
    font-size: 13px;
    border-radius: 8px;
    background: #f5f5f5;
    color: #333;
    cursor: pointer;
    transition: background 0.2s ease;
    border: none;
}
.clear-btn:hover {
    background: #e0e0e0 !important;
}

/* ç”¨æˆ·èŠå¤©æ°”æ³¡ï¼ˆå³ä¾§ï¼‰ */
.chatbot-user {
    background: rgba(186,104,200,0.9);
    color: white;
    padding: 12px 18px;
    border-radius: 22px 22px 0 22px;
    max-width: 70%;
    margin-left: auto;  /* é å³ */
    margin-bottom: 14px;
    word-wrap: break-word;
    font-size: 16px;
    line-height: 1.4;
    box-shadow: 0 4px 10px rgba(0,0,0,0.15);
}

/* åŠ©æ‰‹èŠå¤©æ°”æ³¡ï¼ˆå·¦ä¾§ï¼‰ */
.chatbot-assistant {
    background: rgba(255,255,255,0.85);
    color: #333;
    padding: 12px 18px;
    border-radius: 22px 22px 22px 0;
    max-width: 70%;
    margin-right: auto; /* é å·¦ */
    margin-bottom: 14px;
    word-wrap: break-word;
    font-size: 16px;
    line-height: 1.4;
    box-shadow: 0 4px 10px rgba(0,0,0,0.12);
}


/* æ€è€ƒå— */
.thought-process {
    font-style: italic;
    color: #7b7b7b;
    margin-left: 16px;
    margin-top: 6px;
    margin-bottom: 6px;
    border-left: 3px solid #ba68c8;
    padding-left: 8px;
    background-color: rgba(255,255,255,0.4);
    border-radius: 4px;
    font-size: 14px;
}
"""
    
def clear_all():
    memory.clear()  # æ¸…ç©º langchain å†…å­˜
    return [], []
    
def toggle_kb_fn(current_state: bool):
    new_state = not current_state
    label = "çŸ¥è¯†åº“: å¼€å¯" if new_state else "çŸ¥è¯†åº“: å…³é—­"
    return new_state, label


    
with gr.Blocks(css=css_style) as demo:
    with gr.Column(elem_id="main-container"):
        gr.Markdown("### ğŸŒ¸ æœ¬é—´å¿ƒé“ƒ - æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Galgameé£æ ¼)")

        chatbot = gr.Chatbot(elem_id="chatbot", label="å¯¹è¯", type="messages")

        # è¾“å…¥æ¡† + å‘é€æŒ‰é’®ï¼ˆæ‚¬æµ®åœ¨è¾“å…¥æ¡†å†…éƒ¨ï¼‰
        with gr.Row():
            with gr.Column(elem_classes="input-container"):
                msg = gr.Textbox(show_label=False, placeholder="å’Œå¿ƒé“ƒè¯´ç‚¹ä»€ä¹ˆå§â€¦", lines=1)
                send = gr.Button("â¤", elem_classes="send-btn")

        # æŒ‰é’®åŒºï¼šæ¸…ç©º + çŸ¥è¯†åº“åˆ‡æ¢
        with gr.Row():
            clear = gr.Button("æ¸…ç©ºå¯¹è¯", elem_classes="clear-btn")
            toggle_kb = gr.Button("çŸ¥è¯†åº“: å¼€å¯", elem_classes="clear-btn")

        # çŠ¶æ€
        state = gr.State([])       # èŠå¤©å†å²
        kb_state = gr.State(True) # çŸ¥è¯†åº“çŠ¶æ€

        send.click(fn=chatbot_interface, inputs=[msg, state, kb_state], outputs=[chatbot, state, msg])
        msg.submit(fn=chatbot_interface, inputs=[msg, state, kb_state], outputs=[chatbot, state, msg])


        clear.click(clear_all, None, [chatbot, state, kb_state], queue=False)
        toggle_kb.click(fn=toggle_kb_fn, inputs=[kb_state], outputs=[kb_state, toggle_kb])


demo.launch(server_name="0.0.0.0", server_port=7860)




