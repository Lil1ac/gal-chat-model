{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62d88fa68e54aee8473a08bf17b7238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"thu-coai/lccc\",\"large\",cache_dir='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialog'],\n",
       "        num_rows: 12007759\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialog': ['火锅 我 在 重庆 成都 吃 了 七八 顿 火锅',\n",
       "  '哈哈哈哈 ！ 那 我 的 嘴巴 可能 要 烂掉 ！',\n",
       "  '不会 的 就是 好 油腻']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理 train 数据集...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88824cdc29a345939d78da7a5bc1eaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12007759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93e879133584adc81c30bd5d4cde5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12007759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始对话数量: 12007759\n",
      "处理后对话数量: 66222\n",
      "前3个示例：\n",
      "['你自卑吗', '我不是颜控我是钱控', '钱控说明什么', '说明你欠我钱', '有欠条吗', '木有欠条就不还了嘛', '没欠条也算欠钱吗', '给你两分钟赶快写欠条，不然我就哭给你看', '有本事你哭', '有本事你哄我', '没本事！哭就哭！', '那你先哭。。。', '我为什么要哭', '你不哭，我怎么知道怎么哭', '我也不知道怎么哭', '你个来，我打两下就好了']\n",
      "\n",
      "['我看到了你的照片了', '说说你判断的依据是什么', '长得丑', ':-)谢谢', '我说难听的话，你还谢谢我。', '...呵呵我就知道你会这么说', '睡觉了？', '睡着了就什么也不想了', '不理我，那我就睡觉了', '你看大家都没发话呢', '我累了，明天还要上班，', '看出来了，你眼袋都快拖地了，赶紧睡觉吧']\n",
      "\n",
      "['谢谢关心', '这么客气', '那该怎么说……', '你带情绪…', '换你能不带情绪吗，除非不是真感情', '哎～～', '没什么拉，', '别太计较，会很累，也很不开心，有时候朋友比恋人好', '好啦，知道拉，计较的话又不是这样了，', '嗯嗯，开心起来啦', '还可以吧', '嗯，']\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94be0b9b91494ab4abe263147291e858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/66222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_dialog(example):\n",
    "    # 去掉每句话首尾空格，并用单空格连接词，保证句子整洁\n",
    "    processed = [''.join(sentence.split()) for sentence in example['dialog']]\n",
    "    # 只保留对话轮数≥10的样本\n",
    "    if len(processed) >= 10:\n",
    "        return {'dialog': processed}\n",
    "    else:\n",
    "        return {'dialog': []}  # 空列表，后面过滤掉\n",
    "\n",
    "dataset = load_dataset(\"thu-coai/lccc\",\"large\",cache_dir='./data')\n",
    "\n",
    "processed_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    print(f\"\\n处理 {split} 数据集...\")\n",
    "    processed = dataset[split].map(process_dialog)\n",
    "    processed = processed.filter(lambda x: len(x['dialog']) > 0)\n",
    "    processed_datasets[split] = processed\n",
    "    print(f\"原始对话数量: {len(dataset[split])}\")\n",
    "    print(f\"处理后对话数量: {len(processed)}\")\n",
    "    print(\"前3个示例：\")\n",
    "    for d in processed['dialog'][:3]:\n",
    "        print(d)\n",
    "        print()\n",
    "\n",
    "# 保存数据集\n",
    "for split, ds in processed_datasets.items():\n",
    "    ds.save_to_disk(f\"lccc-clean-{split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后对话数量: 66222\n",
      "前3个示例：\n",
      "['你自卑吗', '我不是颜控我是钱控', '钱控说明什么', '说明你欠我钱', '有欠条吗', '木有欠条就不还了嘛', '没欠条也算欠钱吗', '给你两分钟赶快写欠条，不然我就哭给你看', '有本事你哭', '有本事你哄我', '没本事！哭就哭！', '那你先哭。。。', '我为什么要哭', '你不哭，我怎么知道怎么哭', '我也不知道怎么哭', '你个来，我打两下就好了']\n",
      "\n",
      "['我看到了你的照片了', '说说你判断的依据是什么', '长得丑', ':-)谢谢', '我说难听的话，你还谢谢我。', '...呵呵我就知道你会这么说', '睡觉了？', '睡着了就什么也不想了', '不理我，那我就睡觉了', '你看大家都没发话呢', '我累了，明天还要上班，', '看出来了，你眼袋都快拖地了，赶紧睡觉吧']\n",
      "\n",
      "['谢谢关心', '这么客气', '那该怎么说……', '你带情绪…', '换你能不带情绪吗，除非不是真感情', '哎～～', '没什么拉，', '别太计较，会很累，也很不开心，有时候朋友比恋人好', '好啦，知道拉，计较的话又不是这样了，', '嗯嗯，开心起来啦', '还可以吧', '嗯，']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "processed_dataset = load_from_disk(\"./lccc-clean-train\")  # 路径改成你保存的路径\n",
    "\n",
    "print(f\"处理后对话数量: {len(processed_dataset)}\")\n",
    "print(\"前3个示例：\")\n",
    "for i in range(3):\n",
    "    print(processed_dataset[i]['dialog'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialog'],\n",
       "    num_rows: 66222\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意事项"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "运行代码前先开启学术加速/梯子\n",
    "\n",
    "只能单卡启动，因为是free模式\n",
    "\n",
    "强行用device_map多卡启动也没用，训练的时候会报错\n",
    "\n",
    "sfttrainer中的 processing_class=tokenizer 改成 tokenizer=tokenizer 因为这里是unsloth包装过的sfttrainer\n",
    "\n",
    "用 python train.py运行，不要用deepspeed运行 （不要用deepspeed --include 'localhost:0' train.py 启动）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
